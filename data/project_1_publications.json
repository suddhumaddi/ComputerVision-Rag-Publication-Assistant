[
   
    {
        "id": "0hkuicWh2tKk",
        "username": "regmi.prakriti24",
        "license": "",
        "title": "Hands on Computer Vision: Build Production-Grade Models in an Hour",
        "publication_description": ":::youtube[Title]{#8em2GBD0H8g}\n--DIVIDER--\n---\n--DIVIDER--# Learning Objectives\n> *In this notebook, we will explore the practical implementations of some primal CV tasks like image classification, image segmentation, and object detection using modern computer vision techniques leveraging some popular pre-trained models.*\n\nBy the end of this session, you will be able to:\n\n1) Understand the applications of image classification, segmentation, and object detection. <br>\n2) Use pre-trained models to perform these tasks with minimal setup. <br>\n3) And, visualize the outputs of pre-trained models for test analysis.\n\n<br>\n--DIVIDER--# Prerequisites\n\nTo ensure participants can fully engage and benefit from this workshop, the following are recommended:\n\n1. **Basic Understanding of Python:**\n   Familiarity with Python programming, including syntax, data structures, and basic libraries like numpy and matplotlib.\n\n2. **Google Account:**\n   You'll need a Google account to access and run the Colab notebook we'll be using during the webinar.\n\n3. **Basic Understanding of Deep Learning:**\n   No advanced expertise needed, but a basic grasp of how CNNs process images would be helpful.\n\nAll required libraries and dependencies are pre-installed in the Colab environment.\n--DIVIDER--:::info{title=\"Webinar Resources\"}\n\n\ud83d\udcdd To follow along with this webinar:\n\n1. Use our [Google Colab Notebook](https://colab.research.google.com/drive/1oGzv7q9PqnlNMj0i0pu2ZtvEi-GkoG4N)\n  - Sign in with your Google account\n  - Click \"Copy to Drive\" to create your own editable version\n  - All required libraries are pre-installed in Colab\n\n2. For later reference, check our [GitHub Repository](https://github.com/readytensor/rt-cv-2024-webinar) which is also linked in the **Models** section of this webinar publication.\n  - Contains complete code base\n  - Additional code examples and resources\n  - Extended documentation\n\nThe presentation slides used in this webinar are also available in the **Resources** section as **\"Ready Tensor Computer Vision Webinar.pdf\"**.\n\nWe recommend using the Colab notebook during the code review section for the smoothest experience!\n:::\n\nNow, let's dive into computer vision!--DIVIDER--\n---\n--DIVIDER--# Image Classification\n\nImage classification is the task of identifying what's in an image by assigning it a label from a set of predefined categories. For example, determining if a photo contains a dog, cat, car, or person.\n\nWhen implementing image classification, you have several approaches:\n1. **Build your own models from scratch** - giving you full control but requiring extensive training data and computational resources\n2. **Use pre-trained models** - leveraging models already trained on large datasets like ImageNet \n3. **Fine-tune pre-trained models on your specific dataset** - combining the best of both worlds\n\nFor most real-world applications, using pre-trained models (approach #2) is the smart choice. These models have already learned to recognize a wide variety of visual features, allowing you to:\n- Get started quickly without extensive training data\n- Save significant time and computing resources\n- Often achieve better results than training from scratch\n\nIn this tutorial, we'll use a pre-trained model to classify images. If you're interested in training a model on your own dataset, check out the resources section for a detailed guide on transfer learning and fine-tuning.\n\nLet's get started! \ud83d\udc47--DIVIDER--**Importing the Libraries**\n\n```python\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport glob as glob\nimport os\nimport cv2\nimport random\nimport json\nimport numpy as np\nfrom PIL import Image\n```\n<br> \n--DIVIDER--**Accessing the Data**\n\n```python\nbase_dataset_path = os.path.join(\"WebinarContent\", \"Datasets\")\nclassification_data_samples = \"ImageClassification\"\n\nimages = os.path.join(base_dataset_path, classification_data_samples)\n\nimage_paths = [os.path.join(base_dataset_path, classification_data_samples, x) for x in os.listdir(images) ]\n\n# Sort files for consistent ordering\nimage_paths.sort()\n```--DIVIDER--**Visualizing Test Sample**\n\nWe will use the French Bulldog image for prediction. Let's load and display it.\n\n```python\nplt.figure(figsize=(8,4))\nimage = plt.imread(image_paths[2])\nplt.imshow(image)\nplt.axis(\"off\")\n```--DIVIDER--![FrenchBullDog.jpg](FrenchBullDog.jpg)--DIVIDER--<br>\n\n## Inception V3 for Image Classification\n\nInceptionV3, introduced by **Google** in 2015, is a successor to InceptionV1 and V2. It is a convolutional neural network designed for high accuracy in image classification while being computationally efficient. The model uses convolutional, pooling, and inception modules, with inception blocks enabling the network to learn features at multiple scales using filters of varying sizes.\n\nBefore we move ahead let's take a look at the images the model has been trained on.\n--DIVIDER--**Accessing Inception V3 Model Labels**--DIVIDER--```python\nclass_index_file = \"WebinarContent/ModelConfigs/imagenet_class_index_file.json\"\nwith open(class_index_file, 'r') as f:\n    class_mapping = json.load(f)\n```\n--DIVIDER--```python\nclass_names = [class_mapping[str(i)][1] for i in range(len(class_mapping))]\nprint(f\"Total Classes: {len(class_names)}\")\n```\n```bash\n> Total Classes: 1000\n```--DIVIDER--**Visualization of The Classes**--DIVIDER--```python\nrandom.shuffle(class_names)\nnum_rows, num_cols = 2, 3\nfig, ax = plt.subplots(num_rows, num_cols, figsize=(7, 2.5))\nfig.suptitle(\"Sample ImageNet Classes\", fontsize=12)\nfor i, ax in enumerate(ax.flat):\n  if i < len(class_names[:6]):\n      ax.text(0.5, 0.5, class_names[i], ha='center', va='center', fontsize=10)\n      ax.set_xticks([])\n      ax.set_yticks([])\n  else:\n      ax.axis('off')\n```\n\n--DIVIDER--\n![ImageNet Classes.png](ImageNet%20Classes.png)--DIVIDER--InceptionV3 was trained on the **ImageNet** dataset, a large-scale dataset commonly used for image classification tasks, with categories ranging from animals and plants to everyday objects and scenes, which consists of over **1.2 million labeled images** across **1,000 categories**.\n--DIVIDER--### Loading the Inception V3 Model--DIVIDER--```python\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\n```--DIVIDER--```python\ninception_v3_model = InceptionV3(weights='imagenet')\n```--DIVIDER--**Model Input Size Check**\n\nKnowing the image shape is crucial for preprocessing, model compatibility, resource management (memory), and ensuring the model performs optimally with the given data.\n\n1.   **Model Compatibility**: Most models, including InceptionV3, expect input images of a specific shape (e.g., 299x299x3 for InceptionV3). **If the images fed into the model don't match this expected shape, the model will throw an error.** Therefore, knowing the image shape ensures that the images are preprocessed correctly to fit the model\u2019s requirements.\n\n2.   **Data Preprocessing:** Knowing the expected input shape helps in resizing images properly. If an image is too large or too small, resizing it to the required dimensions is necessary for consistent model performance.\n\n3. **Memory and Computational Efficiency:**: The shape of the image affects the amount of memory required to store the data. Larger images (higher resolution) require more memory. For instance, images of shape (299, 299, 3) will take up less memory than images of shape (512, 512, 3)\n\n--DIVIDER--\n```python\nprint(inception_v3_model.input_shape)\n```\n\n```\n> (None, 299, 299, 3) \n```--DIVIDER--Here, the input shape **(None, 299, 299, 3)** means the InceptionV3 model expects input images of size 299x299 pixels with 3 color channels (RGB). This shape is consistent with the pre-trained InceptionV3 model, which is designed to work with color images resized to 299x299 pixels.--DIVIDER--### Image Preprocessing--DIVIDER--```python\nimage_paths[0]\n\n> Datasets\\ImageClassification\\FrenchBullDog.jpg \n\n```\n--DIVIDER----DIVIDER--```python\ntf_image = tf.io.read_file(image_paths[0]) #reading image\ndecoded_image = tf.image.decode_image(tf_image) # decode the image into a tensor\nimage_resized = tf.image.resize(decoded_image, inception_v3_model.input_shape[1:3]) # resizing the image to match the expected input shape of the model\nimage_batch = tf.expand_dims(image_resized, axis = 0) # add an extra dimension to the image\nimage_batch = preprocess_input(image_batch) #preprocess the image to match the input format\n```--DIVIDER--### Prediction With Inception v3--DIVIDER--```python\nmodel_prediction = inception_v3_model(image_batch)\ndecoded_model_prediction = tf.keras.applications.imagenet_utils.decode_predictions(\n        preds = model_prediction,\n        top = 1\n    )\nprint(\"Predicted Result: {} with confidence {:5.2f}%\".format( decoded_model_prediction[0][0][1],decoded_model_prediction[0][0][2]*100))\nplt.imshow(Image.open(image_paths[0]))\nplt.axis('off')\nplt.show()\n```\n\n![out3.png](out3.png)--DIVIDER--> Oops ! Here the sunglasses overruled :(.\n\nBut we can always use a model more specialized for our use case!\n\n--DIVIDER--\n---\n--DIVIDER--## Using a Specialized Pre-trained Model\n\nWhen a pre-trained model doesn't deliver satisfactory results for your specific use case, one option is to fine-tune the model or explore other sources that provide fine-tuned models.\n\nFine-tuning allows you to adapt a model trained on large datasets to perform better on your specific data by updating only the last few layers of the model.\n\nHere are some sources you can utilize:\n\n1.   TensorFlow Hub\n2.   Hugging Face Model Hub\n3.   Keras Applications\n4.   Facebook AI Research\n5.   ReadyTensor's Model Hub\n\nLet's try using one of the fine-tuned models for our purpose.--DIVIDER--```python\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n```--DIVIDER--```python\nimage_processor = AutoImageProcessor.from_pretrained(\"jhoppanne/Dogs-Breed-Image-Classification-V2\")\nmodel = AutoModelForImageClassification.from_pretrained(\"jhoppanne/Dogs-Breed-Image-Classification-V2\")\n```--DIVIDER--This model is a fine-tuned version of `microsoft/resnet-152` on the **Stanford Dogs** dataset, achieving:\n\n```\nLoss: 1.0115\nAccuracy: 84.08 %\n```\nSource : [Model Source](https://huggingface.co/jhoppanne/Dogs-Breed-Image-Classification-V2)\n\n--DIVIDER--```python\nimage = Image.open(\"WebinarContent/Datasets/ImageClassification/FrenchBullDog.jpg\")\ninputs = image_processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\npredicted_class_idx = logits.argmax(-1).item()\nprint(f\"Predicted class: {model.config.id2label[predicted_class_idx]}.\")\nplt.imshow(image)\n```\n\n![out4.png](out4.png)--DIVIDER--\n\n> There you go! Just what we needed :)\n\n--DIVIDER--\n---\n--DIVIDER--# Object Detection\n\nObject Detection is a computer vision task that involves identifying and localizing objects within an image or video. It not only classifies objects but also uses bounding boxes to pinpoint their positions.\n\n**Key Components:** <br>\n1. Localization: Identifies the object\u2019s position with a bounding box.\n2. Classification: Labels the object (e.g., \"dog,\" \"car\").\n3. Confidence Score: The probability that the prediction is correct.\n**Techniques:**\n**Two-Stage Models (e.g., Faster R-CNN):** Generate region proposals first and classify them second, offering high accuracy but slower speeds. <br>\n**One-Stage Models (e.g., YOLO, SSD):** Predict everything in one pass, fast and suitable for real-time applications but may sacrifice some accuracy.--DIVIDER--> Lets try using the YOLO (a pretrained) model from **UltraLytics** for our object detection task\n\n--DIVIDER--:::info{title=\"Info\"}\n**Why Use YOLO from Ultralytics?**  \nUltralytics **YOLO** models are optimized for fast and accurate inferencing, ideal for real-time tasks like object detection and segmentation. Pre-trained models can be deployed on edge devices and support formats like ONNX and TensorFlow Lite for versatile usage. So, we look forward to leveraging it.\n:::--DIVIDER--### YOLOv11 for Object Detection--DIVIDER--**Loading Libraries**\n\n```python\n#!pip install ultralytics\n```--DIVIDER--**Loading the YOLO Module**\n\n```python\nfrom ultralytics import YOLO\n```--DIVIDER--### Loading The Model\n\n```python\nyolo11_model = YOLO(os.path.join(\"WebinarContent\", \"Models\", \"yolov11m.pt\"))\n```--DIVIDER--**Visualization of The Classes**--DIVIDER--```python\nyolo11_classes = yolo_classes = list(yolo11_model.names.values())\nrandom.seed(0)\nrandom.shuffle(yolo11_classes)\nnum_rows, num_cols = 3, 6\nfig, ax = plt.subplots(num_rows, num_cols, figsize=(8, 2))\nfig.suptitle(\"Sample Yolo11 Classes\", fontsize=12)\nfor i, ax in enumerate(ax.flat):\n  if i < len(yolo11_classes[:18]):\n      ax.text(0.5, 0.5, yolo11_classes[i], ha='center', va='center', fontsize=8)\n      ax.set_xticks([])\n      ax.set_yticks([])\n  else:\n      ax.axis('off')\n```--DIVIDER--\n![sample_yolo_classes.png](sample_yolo_classes.png)--DIVIDER--**Visualizing Test Sample**\nWe will use a road traffic image for object detection. Let's visualize it first.--DIVIDER--```python\ntest_image_path = \"/content/WebinarContent/Datasets/ObjectDetection/roadTraffic.png\"\n```--DIVIDER--```python\ntest_image = Image.open(test_image_path)\nig, ax = plt.subplots()\nax.imshow(test_image)\n```\n\n![out6.png](out6.png)--DIVIDER--### Making Predictions With YOLO v11--DIVIDER--```python\nresults = yolo11_model.predict(test_image_path)\n```\nimage 1/1 WebinarContent\\Datasets\\ObjectDetection\\roadTraffic.png: 352x640 3 persons, 8 cars, 339.6ms\nSpeed: 4.0ms preprocess, 339.6ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n\n--DIVIDER--```python\nprint(f\"The number of objects detected in the image is:{len(results[0].boxes)}\")\n```\nThe number of objects detected in the image is: 11--DIVIDER--### Visualizing Object Detection Results \n--DIVIDER--```python\nprediction_ccordinates = []\npredictions = []\nfor box in results[0].boxes:\n  class_id = results[0].names[box.cls[0].item()]\n  predictions.append(class_id)\n  cords = box.xyxy[0].tolist()\n  cords = [round(x) for x in cords]\n  prediction_ccordinates.append(cords)\n  conf = round(box.conf[0].item(), 2)\n```\n--DIVIDER--```python\nfig, ax = plt.subplots()\nax.imshow(test_image)\nfor i, bbox in enumerate(prediction_ccordinates):\n    rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n                         linewidth=2, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n\n    ax.text(bbox[0], bbox[1] - 10, f'{predictions[i]}', color='b', fontsize=6,\n           backgroundcolor='none')\n\nplt.show()\n```\n\n![out7.png](out7.png)--DIVIDER--This way using a pre-trained **YOLOv8** model for **car detection** in traffic management we can gain several benefits:\n\n1. **Automatic Traffic Analysis**: It can count the number of vehicles, detect traffic jams, and measure the speed of cars, enabling smart traffic lights and dynamic traffic management.\n\n2. **Parking Management**: YOLOv8 can help in detecting available parking spots by identifying parked cars in parking lots, improving the user experience in urban areas.\n\n  And more\n\nThese applications can significantly enhance traffic management, improve road safety, and optimize urban planning.--DIVIDER--\n\n\n> **Let's try to take a level up next !**\n\n--DIVIDER--\n---\n--DIVIDER--# Image Segmentation\nThis is an advanced use case where the model is applied to segment objects in an image, rather than just detecting them. Unlike traditional object detection, segmentation involves classifying each pixel in an image, allowing precise boundaries for objects like cars, people, or building.\n\nThe training of **Object Detection** and **Image Segmentation** models differs mainly in the output and data requirements. Object detection models, like YOLO, produce **bounding boxes** around objects and assign class labels, requiring annotations that specify object locations. Segmentation models, like YoloV8-Seg, generate **pixel-wise masks**, assigning a class to each pixel in the image, requiring more detailed pixel-level annotations.\n\nWhile object detection typically uses simpler loss functions (e.g., bounding box and classification loss) and is less computationally expensive, image segmentation is more resource-intensive, requiring more complex models and loss functions (e.g., Dice loss) to provide precise object boundaries.--DIVIDER--### Loading the YOLOv11-seg Model--DIVIDER--```python\nsegmentation_model = YOLO(os.path.join(\"WebinarContent\", \"Models\", \"yolov11m-seg.pt\"))\n```--DIVIDER--**Loading The Test Image**--DIVIDER--```python\nsegmentation_test_image_path = os.path.join(\"WebinarContent\", \"Datasets\", \"ImageSegmentation\", \"beatles.png\")\nimg = cv2.cvtColor(cv2.imread(segmentation_test_image_path,cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\nplt.imshow(img)\n```\n\n![out8.png](out8.png)--DIVIDER--**Accessing the Model Labels**--DIVIDER--```python\nyolo_seg_classes = list(segmentation_model.names.values())\nclasses_ids = [yolo_classes.index(clas) for clas in yolo_seg_classes]\n```--DIVIDER--### Inferencing With YOLOv11-seg--DIVIDER--```python\nconf = 0.5 # setting threshold\nresults = segmentation_model.predict(img, conf=conf)\n```--DIVIDER--### Visualizing Segmentation Results--DIVIDER--```python\ncolors = [random.choices(range(256), k=3) for _ in classes_ids]\nperson_class_id = 0\n\nfor result in results:\n  for mask, box in zip(result.masks.xy, result.boxes):\n      points = np.int32([mask])\n\n      class_id = int(box.cls[0])\n      if (class_id == person_class_id ):\n        cv2.polylines(img, points, True, (255, 0, 0), 1)\n        color_number = classes_ids.index(int(box.cls[0]))\n        cv2.fillPoly(img, points, colors[color_number])\nplt.imshow(img)\n```\n\n![out9.png](out9.png)--DIVIDER--Voila ! You did it!--DIVIDER--# Conclusion \nAs we have demonstrated in this hands-on session, building production-grade computer vision systems is now achievable within an hour thanks to pre-trained models like InceptionV3 and YOLO. By leveraging these powerful models, we can quickly implement complex tasks from image classification to segmentation, making advanced computer vision capabilities readily accessible for real-world applications.\n--DIVIDER--# Exercises\n\nHere are some exercises to help you practice and extend what you've learned. They are arranged in increasing order of difficulty:\n\n## 1. Model Comparison (Beginner)\nTry using ResNet50 instead of InceptionV3 for image classification:\n- Load the pre-trained ResNet50 model\n- Run inference on the same images\n- Compare the predictions and confidence scores\n- Which model performs better for our dog breed images?\n\n## 2. YOLO Performance Analysis (Intermediate)\nExperiment with different YOLO model sizes:\n- Try all 5 variants\n- Measure and compare inference times and GPU memory usage\n- Analyze the trade-off between speed and accuracy\n- Which size would you choose for a real-time application?\n\n## 3. Object Tracking (Intermediate)\nImplement object tracking in a video:\n- Use YOLO's tracking feature with ByteTrack\n- Display unique IDs for each detected object\n- Track objects across frames\n- Bonus: Add motion trails that fade over time (last 1-2 seconds of movement)\n\n## 4. Video Segmentation with Tracking (Advanced)\nCombine segmentation and tracking in a video pipeline:\n- Load and process video files frame by frame\n- Apply segmentation to each frame\n- Track segmented objects across frames\n- Create an output video showing both masks and tracking IDs\n\nTips and starter code for each exercise are available in the GitHub repository. Feel free to share your project work in a publication on Ready Tensor!--DIVIDER--\n<br>\n\n### Additional Reading Materials \n- Detailed overview on [AlexNet](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), [Inception](https://arxiv.org/pdf/1409.4842), [YOLO](https://arxiv.org/pdf/1506.02640) \n- References to some popular data hubs [IEEE DataPort](https://ieee-dataport.org/datasets), [Hugging Face Dataset Hub](https://huggingface.co/datasets), [Ready Tensor Dataset Hub](https://app.readytensor.ai/datasets)\n- Guidelines to getting started with Frameworks : [Tensorflow](https://www.tensorflow.org/api_docs), [Pytorch](https://pytorch.org/docs/stable/index.html), [Ultralytics](https://www.ultralytics.com/)\n\n\n"
    }
    
]